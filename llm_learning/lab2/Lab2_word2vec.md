# Lab2 Word2Vec

CBDW

什么叫做一个单词的onehot？

Word2Vec structure

CBDW

Skip-gram Model



n-gram:统计各种词串出现的次数并平滑化处理。

在机器学习中，采用最大对数似然来求得一组最优的参数，利用这组参数对应的模型来进行预测。

**神经概率语言模型**

词向量：把一个词表示成一个向量，

常见的编码方式：One-hot representation and Distributed representation

One-hot representation

用一个很长的向量来表示一个词，向量的长度就是词典的大小，其中1的位置就对应该词在词典中出现的位置。

可以体现词语之间的相似性





word embedding and word2vec



为每个词分配不同的数字，以表达不同的语义信息。相似语义的词具有相似的嵌入。

哪一个词输入，我们将它标签为1(one-hot)， 其余词对应标签为0。

我们希望预测的下一个词汇对应的值最大。



通过负采样来增加速度

## Word2Vec structure

### CBOW（连续词袋）

通过上下文来预测中心词。

### Skip-gram

通过中心词来预测其上下文。

**隐藏层**

- 输入向量与*输入权重矩阵** 相乘。
- 这个矩阵相乘的操作，本质上就是**查表**：从输入矩阵中**查找**出对应中心词的那一行（因为One-Hot向量只有一位为1），这一行就是一个*D*维的稠密向量，也就是我们最终想要的**中心词向量**。
- 这个 *D* 维向量直接作为隐藏层的输出。所以，**隐藏层就是中心词的词向量本身**。

**输出层**

- 隐藏层的向量会与一个 D*×*V* 的**输出权重矩阵 **相乘，得到一个 *V* 维的输出向量。
- 这个 *V* 维向量的每一个元素，代表了词汇表中每一个词作为当前中心词的“上下文词”的得分。
- 然后，我们期望通过一个Softmax函数，将这个得分向量转换成一个概率分布。理想情况下，在真实上下文词位置上的概率应该尽可能高。

**训练目标****

对于一个给定的中心词$w_t$和一个特定的上下文词 $w_{t+k}$（在窗口内），模型的目标是最大化在输出层看到 $w_{t+k}$的概率。整个训练过程就是调整两个权重矩阵的参数，使得对于所有训练样本（中心词-上下文词对），这个概率之和最大。

## 关键原理

### down sampling（负采样）

#### 基本原理

为了解决原始的Word2Vec模型在最终输出时巨大的Softmax计算，负采样为每个训练样本（由一个中心词和上下文词组成的正样本）构造一个更简单的**二分类**任务。

模型只需要学习和区分这个中心词与正样本词相关，而与那些随机抽样的负样本词不相关。**噪声对比估计**

$\log\sigma(v_o^Tv_i)+\sum_{j=1}^k\mathbb{E}_{w_j\sim P_n(w)}[\log\sigma(-v_j^Tv_i)]$

> - $v_o$是**正样本**的输出向量
> - $v_j$是**负样本**的输出向量

最终的目标视作最大化这个对数似然。可以直观理解，模型同时在学习做两件事情：

1. **拉近**$v_i$和**正样本**$v_o$的距离
2. **推远**$v_i$和**负样本**$v_j$的距离

#### 如何完成采样？

采用**加权采样**策略，一个词被采样的概率与其在语料库中的**词频**有关。

$P(w_i)=\frac{f(w_i)^{3/4}}{\sum_{j=0}^Vf(w_j)^{3/4}}$

公式中，$f(w_i)$是词$w_i$的频率。

原论文中选择设置参数$\alpha=0.75$是出于经验的一种折中方案：它既相对提升了低频词被采样的概率（$\alpha$没有设置太大），同时也降低了高频词被采样的概率（$\alpha$没有设置太小）。

#### 优势

1. 提升计算效率
2. 生成高质量的“词嵌入”

### Hierarchical Softmax

Huffman树

### 词向量

**相似性**

通过向量的点积来衡量，语义相似的词往往有着相似的嵌入（向量距离很近）

**类比关系**

### Embedding matrix and Context matrix

#### Embedding matrix

[embedding_size, vocab_size]

#### Context matrix

[embedding_size, vocab_size]

## Questions

1. Word2Vec两个算法模型的原理是什么，网络结构怎么画？

2. 网络输入输出是什么？隐藏层的激活函数是什么？输出层的激活函数是什么？

3. 目标函数/损失函数是什么？

	基于Softmax的交叉熵损失函数

	使用负采样后的目标函数

4. Word2Vec如何获取词向量？

	输入权重矩阵，词嵌入

5. 推导一下Word2Vec参数如何更新

6. Word2Vec的两个模型哪个效果好，哪个速度快？为什么？

	Skip-gram效果更好。

	CBOW训练速度更快。

7. Word2Vec加速训练的方法有哪些？

	负采样、层次Softmax、自适应学习率等。

8. 介绍下Negative Sampling,对词频低的和词频高的单词有什么影响？为什么？

	用随机选择的少量负样本代替完整的Softmax计算。

## Referece

[Word2Vec是如何得到词向量的？](https://www.zhihu.com/question/44832436)

[深入浅出Word2Vec原理解析](https://zhuanlan.zhihu.com/p/114538417)

[word2vec——StartQuest](https://www.bilibili.com/video/BV1Km421u7uu/?spm_id_from=333.337.search-card.all.click&vd_source=47dbec3f3db6a86044a31f482a95d4f0)

