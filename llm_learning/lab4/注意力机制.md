# 注意力机制

## 自注意力机制 (Self-Attention)

**核心思想**

自注意力让序列中的每个位置都能够关注到序列中的所有其他位置，从而捕捉长距离依赖关系。

**数学公式**

$Attention(Q,K,V) = softmax(\frac{QK^T} { \sqrt{d_k}})V$

## 多头自注意力(Multi-Head Attention)

### 为什么需要多头？

不同的头可以学习关注不同类型的信息，捕捉词语之间多种不同的语义关系，且可利用GPU并行计算，提高效率。

**数学公式**

$MultiHead(Q, K, V) = Concat(head_1,...,head_h)W^O$

$head_i=Attention(QW_i^Q, KW_i^K, VW_i^V)$

## Reference

[注意力机制 · GitBook](https://www.rethink.fun/chapter15/注意力机制.html)

[Transformer & BERT：李宏毅深度学习2021 PPT](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/self_v7.pdf)